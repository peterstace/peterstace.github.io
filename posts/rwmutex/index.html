<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>When to use Go&#39;s RWMutex</title>
    <link href="/main.css" rel="stylesheet">
  </head>
  <body>
    <div id="page" style="max-width: 50em; margin: auto">
      <div class="bookend">
        <ul>
          <li><a href="/">About</a></li>
          <li><a href="/posts">Posts</a></li>
          
        </ul>
      </div>
      <hr/>
      <div>
        
<h1>When to use Go&#39;s RWMutex</h1>
<div>2022-09-30</div>
<div><p>The Go programming language has two flavours of mutex that can be used to
serialise access to shared state. They are
<a href="https://pkg.go.dev/sync#Mutex"><code>sync.Mutex</code></a> and
<a href="https://pkg.go.dev/sync#RWMutex"><code>sync.RWMutex</code></a>. This blog post explains the
differences between them and quantitatively analyses why <code>RWMutex</code> may not
give performance benefits over <code>Mutex</code> even under read-heavy data access
scenarios.</p>
<h2 id="the-mutex-is-a-classic-synchronisation-primitive">The mutex is a classic synchronisation primitive</h2>
<p>A mutex is a synchronisation primitive that can be used to help ensure only a
single thread of execution can access a critical section at any one time. A
mutex can be &ldquo;locked&rdquo;, and cannot be locked again until it is first &ldquo;unlocked&rdquo;
(typically by the process that locked it). A critical section is a piece of
code that reads or modifies state that <em>must</em> be accessed by a single thread of
execution at a time to maintain its invariants. In Go, this is implemented as
the <code>Mutex</code> type.</p>
<p>Go also contains a read/write mutex variant, implemented as the <code>RWMutex</code>
type. It can be held by either a single writer or multiple readers.</p>
<p><code>Mutex</code> and <code>RWMutex</code> are both <em>advisory locks</em>.  This means that it&rsquo;s up to
the user to ensure that the protected state is only read or modified while the
mutexes are locked. For <code>RWMutex</code>, it&rsquo;s also up to the user to ensure that the
correct locking mode is used (i.e. that the state is <em>not</em> modified if the lock
is only held in read-mode).</p>
<h2 id="rwmutex-_might_-be-better-in-specific-circumstances"><code>RWMutex</code> <em>might</em> be better in specific circumstances</h2>
<p>In situations where operations read but do not modify state, <code>RWMutex</code> looks as
though it may improve performance over <code>Mutex</code>. If using a regular <code>Mutex</code>,
readers have to wait for others to finish before they get their turn (access is
completely serial).  But with an <code>RWMutex</code>, many readers can access the
critical section simultaneously (read access is completely concurrent).  If
readers aren&rsquo;t waiting for each other, then it follows that execution may be
faster.</p>
<p>The amount of performance improvement expected when replacing a <code>Mutex</code> with an
<code>RWMutex</code> should intuitively be higher in the following scenarios:</p>
<ul>
<li>
<p><strong>Long critical sections.</strong> This is because other readers will take longer to
finish their turn.</p>
</li>
<li>
<p><strong>Frequent access rate</strong> (high events per second).  This is because there is
a higher chance that the lock will already be held whenever a reader tries to
acquire it. Even if the critical section is very short, if there are many
readers then any particular reader may have to wait a long time.</p>
</li>
<li>
<p><strong>High CPU resource availability.</strong> Assuming that the critical section is CPU
bound, it&rsquo;s only going to be possible to execute as many readers
simultaneously as there are CPUs available.</p>
</li>
</ul>
<h2 id="mutex-contention-can-be-modelled-stochastically">Mutex contention can be modelled stochastically</h2>
<p>The effect of &ldquo;long critical sections&rdquo; and &ldquo;highly frequent access rate&rdquo; need
to be quantified in order to be useful. This can be done by simulating various
access scenarios using a <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte
Carlo</a> simulation.</p>
<p>First, let&rsquo;s make some assumptions in order to create a simple model:</p>
<ul>
<li>
<p><strong>Assume many readers and <em>no</em> writers.</strong> This is a crude approximation for a
read-heavy system, such as a web server that only serves static content.</p>
</li>
<li>
<p><strong>Assume no overhead due to locking/unlocking, and coordination/switching
between execution threads.</strong> These overheads are small, so this is a
reasonable simplification.</p>
</li>
<li>
<p><strong>Assume readers arrive at random times, independent of each other</strong>. This is
a reasonable assumption if reads are initiated by user actions and there are
many different users.</p>
</li>
<li>
<p><strong>Assume there are sufficient CPU resources</strong> to allow an unbounded number of
readers to execute the critical section in parallel. This is unrealistic but
only stands to benefit the <code>RWMutex</code>.</p>
</li>
</ul>
<p>There are two parts of the model that we can vary during the Monte Carlo
simulation:</p>
<ul>
<li>
<p>The overall access rate, in events per second.</p>
</li>
<li>
<p>The critical section length, as a duration.</p>
</li>
</ul>
<p>The simulation output is the time elapsed between event initiation and
event completion. This is the sum of waiting time and the duration of the
critical section itself. Many read events will be simulated, so it&rsquo;s most
useful to work with aggregates, such as the 50th (median), 95th, and 99th
percentiles.</p>
<p>For an <code>RWMutex</code>, the model is so trivial that there is no need to simulate it.
The total time elapsed is the same as the critical section duration.
This is because of the assumptions made for the model. Readers don&rsquo;t need to
wait before accessing the critical section, and an unlimited number of readers
can progress simultaneously.</p>
<p>For a <code>Mutex</code>, things are more complicated, because waiting time needs to be
simulated. This is where a Monte Carlo simulation is helpful. The simulation is
implemented in Go, but the code snippets below are annotated so that they&rsquo;re
accessible to those not familiar with the language.</p>
<p>First, we simulate a large, fixed number of events (say, 10 million).</p>
<pre tabindex="0"><code>const totalEvents = 10_000_000
</code></pre><p>Next, we calculate the total duration of the simulation. Because one of the
inputs to the simulation is events per second, this is a simple division:</p>
<pre tabindex="0"><code>totalDurationSec := totalEvents / eventsPerSec
</code></pre><p>The arrival time of each event is then independently chosen such that events
are evenly distributed throughout the length of the simulation:</p>
<pre tabindex="0"><code>eventStarts := make([]float64, totalEvents)
for i := range eventStarts {
	// rand.Float64() is between 0 and 1
	eventStarts[i] = rand.Float64() * totalDurationSec
}
sort.Float64s(eventStarts)
</code></pre><p>The waiting time for each read event can then be simulated, by keeping track of
when the critical section is next unlocked.</p>
<pre tabindex="0"><code>var blockedUntil float64
eventDurations := make([]float64, len(eventStarts))
for i, start := range eventStarts {
	if blockedUntil &lt;= start {
		// The event is unblocked so can run straight away.
		blockedUntil = start + crititalSectionLengthSec
	} else {
		// The event is blocked, so can&#39;t run immediately.
		blockedUntil += crititalSectionLengthSec
	}
	eventDuration := blockedUntil - start
	eventDurations[i] = eventDuration
}
</code></pre><p>After the simulation has completed, percentiles on the distribution of read
event durations can be calculated (code omitted due to being obvious and
uninteresting).</p>
<p>The unabridged code listing is below:</p>
<details>
<summary><code>main.go</code></summary>
<pre tabindex="0"><code>package main

import (
	&#34;encoding/json&#34;
	&#34;flag&#34;
	&#34;math/rand&#34;
	&#34;os&#34;
	&#34;sort&#34;
	&#34;time&#34;

	&#34;golang.org/x/exp/slices&#34;
)

func main() {
	criticalSectionLength := flag.Duration(
		&#34;critical-section-length&#34;,
		500*time.Microsecond,
		&#34;length of the simulated critical section length&#34;)
	eventsPerSec := flag.Int(
		&#34;events-per-second&#34;,
		1_000,
		&#34;number of requests to access the critical section per second&#34;)
	flag.Parse()
	Simulate(criticalSectionLength.Seconds(), float64(*eventsPerSec))
}

func Simulate(crititalSectionLengthSec float64, eventsPerSec float64) {
	const totalEvents = 10_000_000
	totalDurationSec := totalEvents / eventsPerSec

	eventStarts := make([]float64, totalEvents)
	for i := range eventStarts {
		eventStarts[i] = rand.Float64() * totalDurationSec
	}
	sort.Float64s(eventStarts)

	var blockedUntil float64
	eventDurations := make([]float64, len(eventStarts))
	for i, start := range eventStarts {
		if blockedUntil &lt;= start {
			// The event is unblocked so can run straight away.
			blockedUntil = start + crititalSectionLengthSec
		} else {
			// The event is blocked, so can&#39;t run immediately.
			blockedUntil += crititalSectionLengthSec
		}
		eventDuration := blockedUntil - start
		eventDurations[i] = eventDuration
	}

	dist := distribution(eventDurations)

	output := map[string]any{
		&#34;crit&#34;: crititalSectionLengthSec,
		&#34;eps&#34;:  eventsPerSec,
		&#34;p50&#34;:  dist.p50 / crititalSectionLengthSec,
		&#34;p95&#34;:  dist.p95 / crititalSectionLengthSec,
		&#34;p99&#34;:  dist.p99 / crititalSectionLengthSec,
		&#34;avg&#34;:  mean(eventDurations) / crititalSectionLengthSec,
	}
	json.NewEncoder(os.Stdout).Encode(output)
}

type percentiles struct {
	p50 float64
	p95 float64
	p99 float64
}

func distribution(durations []float64) percentiles {
	slices.Sort(durations)
	n := len(durations)
	return percentiles{
		p50: durations[n*50/100],
		p95: durations[n*95/100],
		p99: durations[n*99/100],
	}
}

func mean(durations []float64) float64 {
	var sum float64
	for _, d := range durations {
		sum += d
	}
	n := float64(len(durations))
	return sum / n
}
</code></pre></details>
<h2 id="readwrite-mutexes-only-give-benefit-in-rare-circumstances">Read/write mutexes only give benefit in rare circumstances</h2>
<p>The graph below shows the result of the Monte Carlo simulation for various
scenarios. The colours in the graph show the theoretical slowdown of <code>RWMutex</code>
compared to <code>Mutex</code>, from 1.0 (no slowdown) up to 2.0 (twice as slow) at the
99th percentile of the distribution.</p>
<p><img src="monte_carlo.svg" alt="Monte Carlo Simulation"></p>
<p>The graph confirms that mutex contention is highest (and therefore <code>RWMutex</code> is
most useful!) when the arrival rate is high or the critical section has a long
duration.</p>
<p>The graph shows something else interesting. The transition from &ldquo;no contention&rdquo;
to &ldquo;high contention&rdquo; follows a straight line. If some values at the transition
are taken and their product calculated, it&rsquo;s clear that the product is always a
constant.</p>
<table>
<thead>
<tr>
<th>Arrival Rate</th>
<th>Critical Section Length</th>
<th>Product</th>
</tr>
</thead>
<tbody>
<tr>
<td>100k events/sec</td>
<td>10ns</td>
<td>0.01</td>
</tr>
<tr>
<td>10k events/sec</td>
<td>1us</td>
<td>0.01</td>
</tr>
<tr>
<td>1k events/sec</td>
<td>10us</td>
<td>0.01</td>
</tr>
<tr>
<td>100 events/sec</td>
<td>100us</td>
<td>0.01</td>
</tr>
<tr>
<td>10 events/sec</td>
<td>1ms</td>
<td>0.01</td>
</tr>
</tbody>
</table>
<p>Note that the units of arrival rate (events per time) and critical section
(time per event) cancel out, so the product is unitless. Let&rsquo;s call the product
the <em>contention factor</em>.</p>
<p>The 99th percentile is used for the graph, so we can <em>conservatively</em> say that
<code>RWMutex</code> only starts to have a theoretical performance improvement over
<code>Mutex</code> once the contention factor above (0.01) is reached.</p>
<p>Furthermore, one of the assumptions made was that there are sufficient CPUs
available to execute many simultaneous readers. In particular, if there is only
1 CPU available, then <code>RWMutex</code> is never going to give a performance
improvement.</p>
<p>Here are some example scenarios of what this means:</p>
<ul>
<li>
<p>A single instance of a web service is serving requests at a modest rate of
100 requests per second. As part of serving each request, it performs a
single map lookup, protected by a mutex because the map is <em>occasionally</em>
updated.  Because a map lookup takes somewhere between 10ns and 100ns, the
contention factor is between 0.000001 (100 ⋅ 10 ⋅ 10<sup>-9</sup>) and
0.00001 (100 ⋅ 100 ⋅ 10<sup>-9</sup>). Because this is <em>well</em> below 0.01, an
<code>RWMutex</code> will not provide any benefit over a <code>Mutex</code>.</p>
</li>
<li>
<p>Performing an aggregation operation on a large dataset may take 1ms (for
example, calculating a histogram for 100,000 records). If that is needed to
be done 100 times per second, the contention factor is 0.1 (100 ⋅
10<sup>-3</sup>).  Because this is above 0.01, an <code>RWMutex</code> could
theoretically be expected to provide some benefit over a <code>Mutex</code>.</p>
</li>
</ul>
<h2 id="keep-it-simple-stupid">Keep It Simple, Stupid</h2>
<p>Since <code>RWMutex</code> has the <em>potential</em> to provide a performance improvement, why
not use it by default, and forget about doing a back of the envelope contention
factor calculation?</p>
<p>This is tempting, but I think there are some good reasons to prefer <code>Mutex</code>:</p>
<ol>
<li>
<p>When using <code>RWMutex</code>, it&rsquo;s important to use consistent locking modes
(read-only vs read-write) for each critical section. This seems trivial to
get right, but I&rsquo;ve seen outages on multiple occasions due to bugs relating
to mismatched locking modes.</p>
</li>
<li>
<p>Because locks in Go are <em>advisory</em>, it&rsquo;s up to the programmer to make sure
<code>RWMutex</code>s are held in the correct mode for what each critical section does.
If state is modified while holding the mutex in read-only mode, the program
will run but will be buggy. Again, this seems trivial to get right, but in
practice, I&rsquo;ve seen things go wrong here. The way I&rsquo;ve seen this play out is
that <code>RWMutex</code> is used correctly when <em>code is first written</em>, but then a
write operation is added in the critical section later without changing the
locking mode.</p>
</li>
<li>
<p><code>RWMutex</code> is non-recursive. This means that a critical section should not
acquire the lock in read-only mode a second time, even if the lock would be
released the correct number of times by the end of the critical section. If
an <code>RWMutex</code> is used in this way, a deadlock will occur when another
goroutine concurrently attempts to acquire the <code>RWMutex</code> in read-write
mode. This is explicitly stated in <code>RWMutex</code>&rsquo;s documentation, but of course,
not everyone reads the documentation. Bugs related to recursive usage of
<code>RMutex</code> are particularly pernicious because they&rsquo;re difficult to catch in
unit tests and may exist in production code for a very long time before
causing problems.</p>
</li>
</ol>
<p>My advice is to default to using a <code>Mutex</code> to lock critical sections. If you
suspect there may be contention that an <code>RWMutex</code> could help alleviate,
calculate the contention factor to confirm. If it&rsquo;s 0.01 or higher, use an
<code>RWMutex</code> with the appropriate level of caution.</p>
<h2 id="further-reading">Further reading</h2>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Lock_(computer_science)">Wikipedia: Lock (computer science)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Critical_section">Wikipedia: Critical section</a></li>
<li><a href="https://go.dev/tour/concurrency/9">A tour of Go: concurrency</a></li>
<li><a href="https://pkg.go.dev/sync"><code>sync</code> package documentation</a></li>
</ul>
</div>

      </div>
      <hr/>
      <div class="bookend" style="display: flex">
        <div style="width: 50%; text-align: left">
          <ul>
            <li><a href="https://github.com/peterstace">Github</a></li>
            <li><a href="https://twitter.com/peterjohnstace">Twitter</a></li>
            <li><a href="https://www.linkedin.com/in/petsta/">LinkedIn</a></li>
          </ul>
        </div>
        <div style="width: 50%; text-align: right">
          
&copy; Peter Stace 2022

        </div>
      </div>
    </div>
  </body>
</html>
